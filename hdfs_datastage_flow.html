<h1>Hadoop HDFS on AWS DataStage Flow Tutorial</h1>
<p align="center">
  <img src="img_hdfs/Amazon-Linux2-Logo.png" alt="Amazon Linux 2">
  <img src="img_hdfs/Hadoop_logo_new.svg" alt="Apache Hadoop">
</p>

<h2>About</h2>
<ul>
<li>
<p>This guide provides a detailed walkthrough on setup of Apache HDFS on an AWS EC2 node running Amazon Linux 2. The end result will be a pseudo-distributed Apache Hadoop Distributed File System with WebHDFS enabled, which can connect to 3rd party applications and serve HTTP requests. </p>
</li>
<li>
<p>A single node pseudo-distributed HDFS architecture was chosen so that users of the guide could learn end-to-end Apache HDFS setup without a massive time investment. While Apache HDFS is usually intended to run on many nodes, this guide should be a great start for initial HDFS setup. </p>
</li>
<li>
<p><strong><em>To scale this guide up for more nodes:</em></strong> The fully configured node from this guide should be regarded as your new NameNode. The resulting Apache HDFS architecture can be extended for further nodes by spinning up more EC2 instances to serve as DataNodes (and/or a Secondary NameNode). Each node added will need to be setup with Java, required software packages, environment variables, and completed config (<code>hadoop-env.sh</code>, <code>core-site.xml</code> and <code>hdfs-site.xml</code>) files as the NameNode was. There will be some differences to the properties specified in the <code>core-site.xml</code> and <code>hdfs-site.xml</code> files on the added DataNodes. On the NameNode, the <code>workers</code> file must be edited to contain the DataNode IPs, and the <code>core-site.xml</code> and <code>hdfs-site.xml</code> config properties will require edits to reflect the fully-qualified domain names (FQDNs) of added DataNodes, the new replication count, etc. For more details on setting up fully distributed Apache HDFS, see <a href="https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/ClusterSetup.html">here</a>.</p>
</li>
<li>
<p>Successful DataStage Flows using this guide have been verified for:</p>
</li>
<li>
<p>CP4D on ROSA &lt;&gt; Apache HDFS on AWS EC2 w/ Amazon Linux 2</p>
</li>
<li>CP4D on IBM Cloud &lt;&gt; APache HDFS on AWS EC2 w/ Amazon Linux 2</li>
</ul>
<h2>Table of Contents</h2>
<ul>
<li><a href="#hadoop-hdfs-on-aws-datastage-flow-tutorial">Hadoop HDFS on AWS DataStage Flow Tutorial</a></li>
<li><a href="#about">About</a></li>
<li><a href="#table-of-contents">Table of Contents</a></li>
<li><a href="#1-single-node-basic-configuration">1. Single Node Basic Configuration</a><ul>
<li><a href="#11-create-an-aws-free-tier-account">1.1 Create an AWS Free Tier Account</a></li>
<li><a href="#12-create-an-ec2-template-for-hdfs-nodes">1.2 Create an EC2 Template for HDFS Nodes</a></li>
<li><a href="#13-note-instance-details-and-ssh-to-node">1.3 Note Instance Details and SSH to Node</a></li>
<li><a href="#14-correctly-configure-runtimes-and-software-on-the-node">1.4 Correctly configure runtimes and software on the Node</a></li>
<li><a href="#15-set-up-environment-variables-and-bashrc">1.5 Set up environment variables and .bashrc</a></li>
</ul>
</li>
<li><a href="#4-build-a-data-flow-that-performs-io-with-apache-hdfs">4. Build a Data Flow that Performs I/O with Apache HDFS</a><ul>
<li><a href="#41-build-a-data-flow">4.1 Build a Data Flow</a></li>
<li><a href="#42-verify-successful-etl">4.2 Verify Successful ETL</a></li>
</ul>
</li>
</ul>
<h2>1. Single Node Basic Configuration</h2>
<h3>1.1 Create an AWS Free Tier Account</h3>
<ol>
<li>Point your web browser <a href="https://portal.aws.amazon.com/billing/signup#/start/email">here</a>, and fill out your information to create an AWS free tier account. 
<img src="img_hdfs/AWS_Signup.png" width="60%"></li>
</ol>
<h3>1.2 Create an EC2 Template for HDFS Nodes</h3>
<ol>
<li>
<p>Navigate to the EC2 dashboard within AWS by clicking on <code>Launch a virtual machine</code>. </p>
<p><img src="img_hdfs/go_to_EC2.png" width="80%"></p>
</li>
<li>
<p>Go to the <code>Launch Templates</code> tab in the left sidebar. Here we will create a template to launch our HDFS node(s) from. In this tutorial we will spin up a pseudo-distributed Apache HDFS on a single node, but you may want to scale to more nodes in the future. </p>
</li>
<li>
<p>Click <code>Create Launch Template</code>. </p>
<p><img src = "img_hdfs/click_create_launch_template.png"></p>
</li>
<li>
<p>Give your template a name. I chose the name 'HDFS-Node-Template'.</p>
</li>
<li>
<p>Choose one of the free tier official Amazon Linux 2 Machine Images. It is very important that your node runs on Amazon Linux 2 for compatibility with all of the commands and software packages used in this guide. Other AMIs will work, but may require lots of tinkering. This guide has already resolved the kinks in setup with HDFS 3.3.5 on AL2. Pictured below is the exact image used in this guide. </p>
<p><img src = "img_hdfs/EC2_AMI.png" width="60%"></p>
</li>
<li>
<p>We won't need a ton of compute power or memory. Choose instance type <code>t2.micro</code>.</p>
<p><img src = "img_hdfs/AWS_instance_type.png" width = "68%"></p>
</li>
<li>
<p>In the key-pair section, click <code>Create new key pair</code>, give that key-pair a name, and copy the key-pair into your <code>~/.ssh</code> directory on your local machine, so that you can SSH into your EC2 node later. Here, my key-pair is named <code>hdfs-key1.pem</code>. </p>
<p><img src="img_hdfs/AWS_key_pair.png" width="70%"></p>
<blockquote>
<p>Note: You may need to run <code>chmod 0600 ~/.ssh/hdfs-key1.pem</code> if your key file does not set permissions correctly automatically.</p>
</blockquote>
</li>
<li>
<p>The security group section is critical to making sure that HTTP requests sent to your HDFS cluster work correctly, and for making sure that WebHDFS can be configured. Make sure that inbound traffic is open from all sources. Click <code>Create new security group</code> and configure the settings as below. </p>
<p><img src="img_hdfs/AWS_security_group.png"></p>
<blockquote>
<p>Note: In a production environment you will not want such relaxed security rules. In that case, you can whitelist the specific traffic necessary to make HDFS, WebHDFS, etc. connections work without opening to other traffic. The exact settings will depend on your production environment.</p>
</blockquote>
</li>
<li>
<p>SSD storage with 8 GB capacity should be plenty. The following EBS (Elastic Block Store) storage was used for the guide node's configuration. </p>
<p><img src="img_hdfs/AWS_storage.png" width="70%"></p>
</li>
<li>
<p>Save your template.</p>
</li>
<li>Now, navigate back to the <code>Instances</code> tab from the sidebar.</li>
<li>Click the arrow to the right of <code>Launch instance</code> and select <code>Launch from template</code>. </li>
<li>Select the template you just created (for me, "HDFS-Node-Template"), and confirm that the settings all look correct then click <code>Launch Instance</code>. </li>
</ol>
<h3>1.3 Note Instance Details and SSH to Node</h3>
<ol>
<li>In the <code>Instances</code> tab you should see an instance spinning up. When it finishes spinning up you should see an "Instance State" of "Running" and eventually a successful "Status Check". Click the edit symbol in the "Name" to give your node a name. Here is what my node looks like: <img src="img_hdfs/AWS_instance_running.png"></li>
<li>Click the checkbox on the left of your Instance Name to see important details about your node. Make note of the following, as they will be used throughout the rest of the guide: <code>Public IPv4 address</code>, <code>Private IPv4 addresses</code>, <code>Private IP DNS name (IPv4 only)</code>, and <code>Public IPv4 DNS</code>. 
    &gt; Throughout the rest of the guide, these will be referenced as <code>{Public IPv4 address}</code>, <code>{Private IPv4 addresses}</code>, <code>{Private IP DNS name (IPv4 only)}</code>, and <code>{Public IPv4 DNS}</code> where the corresponding values need to be filled in for config files etc.</li>
<li>
<p>Set up SSH to your Amazon Linux 2 EC2 node from your local machine. First, set an alias for the node's public IP address: </p>
<blockquote>
<p>Feel free to add the above command into the shell initialization file used for your local shell configuration so that you don't need to rerun it each time you restart your shell to have the alias available (ie: <code>~/.bashrc</code> or <code>~/.zshrc</code> most likely).</p>
</blockquote>
<p><code>$ export HDFS_NODE={Public IPv4 address}</code></p>
</li>
<li>
<p>Now, assuming your <code>.pem</code> file is also named <code>hdfs-key1.pem</code>, the following command can be used to ssh into your EC2 node from your local machine: </p>
<p><code>$ ssh -i .ssh/hdfs-key1.pem ec2-user@$HDFS_NODE</code></p>
<blockquote>
<p>Say yes when prompted to add your EC2's Public IP to known hosts.</p>
</blockquote>
</li>
</ol>
<h3>1.4 Correctly configure runtimes and software on the Node</h3>
<blockquote>
<p>Note: Please be very careful following the steps in this section. Lots of time went into finding the correct software versions and configuration to allow Apache HDFS + WebHDFS to run smoothly. </p>
</blockquote>
<ol>
<li>SSH into your node.</li>
</ol>
<p><code>$ ssh -i .ssh/hdfs-key1.pem ec2-user@$HDFS_NODE</code></p>
<ol>
<li>
<p>Download the "extra packages repo", EPEL. This contains several important software packages we need to get WebHDFS running. </p>
<p><code>$ sudo rpm -ivh https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm</code></p>
</li>
<li>
<p>Update the package index, "yum". </p>
<blockquote>
<p>The package index is a tool which keeps track of package names across your downloaded repos so that you can easily download and update linux software packages.</p>
</blockquote>
<p><code>$ sudo yum update -y</code></p>
</li>
<li>
<p>Install openjdk for java 8. This is one of the most highly recommended versions for compatability with HDFS, but any Java 8 will compiled with HDFS. </p>
<p><code>$ sudo yum install java-1.8.0-openjdk-devel -y</code></p>
<p>Then run </p>
<p><code>$ java -version</code> </p>
<p>and you should see the following output: </p>
<pre><code>openjdk version "1.8.0_362" 
OpenJDK Runtime Environment (build 1.8.0_362-b08) 
OpenJDK 64-Bit Server VM (build 25.362-b08, mixed mode)
</code></pre>
</li>
<li>
<p>Install pdsh for better resource management:</p>
<p><code>$ sudo yum install pdsh -y</code></p>
</li>
<li>
<p>Get OpenSSH and start an sshd (ssh server service):</p>
<p><code>$ sudo yum install openssh-server -y</code></p>
<p><code>$ sudo systemctl start sshd</code></p>
</li>
<li>
<p>Download Hadoop (Stable versions can be found <a href="https://www.apache.org/dyn/closer.cgi/hadoop/common/">here</a>).</p>
<p>For the exact version used in this guide, the following command was executed to download hadoop from the web:</p>
<p><code>$ wget https://archive.apache.org/dist/hadoop/common/hadoop-3.3.5/hadoop-3.3.5.tar.gz</code></p>
</li>
<li>
<p>It may take a few minutes to download. Once downloaded, unzip the hadoop distribution as follows: </p>
<p><code>$ tar -xzf hadoop-3.3.5.tar.gz</code></p>
<p>This may also take a minute.</p>
</li>
</ol>
<h3>1.5 Set up environment variables and .bashrc</h3>
<ol>
<li>
<p>First, locate where OpenJDK installed on your machine. To do so, run the following command: </p>
<p><code>$ sudo update-alternatives --config java</code></p>
<p>For my machine, the location was found at: "/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.362.b08-1.amzn2.0.1.x86_64/jre/bin/java".</p>
<p>You will want to set JAVA_HOME to the parent directory of the <code>jre</code> folder, which contains the Java Runtime Environment. </p>
</li>
<li>
<p>To edit <code>.bashrc</code> for your user on the EC2 node (named <code>ec2-user</code> if you used the default username during your EC2 template setup), type the following (you could use vi or vim as well, of course) to edit your bash shell initialization file:</p>
<p><code>$ nano ~/.bashrc</code></p>
</li>
<li>
<p>Now add the following line to the file:</p>
<p><code>$ export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.362.b08-1.amzn2.0.1.x86_64</code></p>
</li>
<li>
<p>Your <code>~/.bashrc</code> file should look like the following now (your install path may be different): </p>
<p>```bash</p>
<h1>.bashrc</h1>
<h1>Source global definitions</h1>
<p>if [ -f /etc/bashrc ]; then
        . /etc/bashrc 
fi</p>
<h1>Uncomment the following line if you don't like systemctl's auto-paging feature:</h1>
<h1>export SYSTEMD_PAGER=</h1>
<h1>User specific aliases and functions</h1>
<h1>Set env vars</h1>
<p>export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.362.b08-1.amzn2.0.1.x86_64
```</p>
</li>
<li>
<p>If using nano to edit the file: Now type <code>^X</code> and <code>y</code> at the prompt to save your changes. </p>
</li>
<li>
<p>Now type the following command to make sure the <code>JAVA_HOME</code> is available in our current bash session.</p>
<p><code>$ source .bashrc</code></p>
<blockquote>
<p>.bashrc will be automatically sourced on each bootup in the future, so you don't need to worry about sourcing <code>JAVA_HOME</code> each time you boot up the node. </p>
</blockquote>
</li>
<li>
<p>Now repeat the process and add environment variables <code>HADOOP_DIR=/path/to/hadoop</code> and <code>HADOOP_CONF_DIR=/path/to/hadoop/conf/files</code> in <code>.bashrc</code> and source it once again. For me these, values were <code>HADOOP_DIR=~/hadoop-3.3.5</code> and <code>HADOOP_CONF_DIR=$HADOOP_DIR/etc/hadoop</code>.</p>
</li>
<li>
<p>Your <code>~/.bashrc</code> file should look like the following now (paths may differ):
   ```bash
    # .bashrc</p>
<h1>Source global definitions</h1>
<p>if [ -f /etc/bashrc ]; then
        . /etc/bashrc 
fi</p>
<h1>Uncomment the following line if you don't like systemctl's auto-paging feature:</h1>
<h1>export SYSTEMD_PAGER=</h1>
<h1>User specific aliases and functions</h1>
<h1>Set env vars</h1>
<p>export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.362.b08-1.amzn2.0.1.x86_64
export HADOOP_DIR=~/hadoop-3.3.5
export HADOOP_CONF_DIR=$HADOOP_DIR/etc/hadoop
``` </p>
</li>
<li>
<p>Source your initialization file again, or restart your shell and re-SSH into the EC2 node: </p>
<p><code>$ source .bashrc</code></p>
</li>
<li>
<p>Change directory into the filepath where your Apache HDFS 3.3.5 distribution lies: </p>
<p><code>$ cd $HADOOP_DIR</code>. </p>
</li>
<li>
<p>We need to edit <code>$HADOOP_DIR/etc/hadoop/hadoop-env.sh</code>. Add the following line (using nano, vi, vim, etc. as you did for editing <code>~/.bashrc</code>). There should be a place in the <code>hadoop-env.sh</code> file commented out waiting for you to uncomment and add the directory location for JAVA_HOME. If you can't find it, feel free to export the JAVA_HOME variable anywhere.</p>
<p>```bash
...</p>
<h1>The java implementation to use. By default, this environment</h1>
<h1>variable is REQUIRED on ALL platforms except OS X!</h1>
<p>export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.362.b08-1.amzn2.0.1.x86_64</p>
<p>...
```</p>
</li>
<li>
<p>At this point, if you run the following command (from within the hadoop distribution directory), you should see a bunch of information pop up on hadoop script usage:</p>
<p><code>$ bin/hdfs</code></p>
</li>
</ol>
<h3>1.6 Set up Node Local Filesystem and add Input Data</h3>
<ol>
<li>
<p>If not already logged into your EC2 instance, SSH back into the EC2 instance:</p>
<p><code>$ ssh -i .ssh/hdfs-key1.pem ec2-user@$HDFS_NODE</code></p>
</li>
<li>
<p>Make an input directory for data you want to input for MapReduce or other data processing such as IBM DataStage later.</p>
<p><code>$ mkdir input</code></p>
</li>
<li>
<p>Do the same for output data. Both of these directories will be used for transfer between the local EC2 node and pseudo-distributed HDFS filesystem.</p>
<p><code>$ mkdir output</code></p>
</li>
<li>
<p>Add data to the input folder. One way to do this is to download Cyberduck to your local computer (not the EC2 node) from <a href="https://cyberduck.io/download/">here</a>.</p>
</li>
<li>
<p>If adding files via Cyberduck, configure a SFTP (Secure File Transfer Protocol) between your local computer and the EC2 node. Select <code>SFTP</code>, copy <code>{Public IPv4 address}</code> into the Server cell, use Port <code>22</code>, and provide the username to log in as. If you did not modify the default EC2 username it should be <code>ec2-user</code>. Finally, specify the SSH Private Key file, which will be <code>~/.ssh/hdfs-key1.pem</code> if you followed the naming conventions in this guide. </p>
<p><img src="img_hdfs/cyberduck_cxn.png" width="70%"></p>
</li>
<li>
<p>Connect to the SFTP you just set up.</p>
</li>
<li>
<p>You should see the local filesystem on the EC2 node, something like the following: </p>
<p><img src="img_hdfs/cyberduck-fs.png" width="80%"></p>
</li>
<li>
<p>Using cyberduck add <a href="https://www.kaggle.com/datasets/kyanyoga/sample-sales-data">this</a> sample sales dataset to the <code>input</code> directory on your EC2 node. Simply drag the unzipped datafile from your local computer into the <code>input</code> directory on the EC2 node in Cyberduck's GUI. </p>
<p><img src="img_hdfs/cyberduck-fs-with-input.png" width="80%"></p>
<blockquote>
<p>I will later use the sales dataset in an IBM DataStage Flow to demonstrate how Apache HDFS can be connected with external tools via WebHDFS.</p>
</blockquote>
</li>
</ol>
<h2>2. Single Node pseudo-distributed HDFS environment</h2>
<p>Hadoop can actually run as a single java process on this one EC2 node, which is good for debug purposes when developing MapReduce scripts. However, this doesn't mimic a more realistic Hadoop file system. We will set up a pseudo-distributed environment with multiple java processes. </p>
<h3>2.1. Psuedo-distributed HDFS config files</h3>
<ol>
<li>
<p>Write the following into your <code>$HADOOP_DIR/etc/hadoop/core-site.xml</code> config file, replacing {Public IPv4 DNS} with the Public IPv4 DNS for your EC2 node:</p>
<p>```xml
&lt;?xml version="1.0" encoding="UTF-8"?&gt;
&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;
&lt;!--
  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at</p>
<pre><code>http://www.apache.org/licenses/LICENSE-2.0
</code></pre>
<p>Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License. See accompanying LICENSE file.
--&gt;</p>
<!-- Put site-specific property overrides in this file. -->
<p><configuration>
  <property>
    <name>fs.defaultFS</name>
    <value>hdfs://{Public IPv4 DNS}:9000</value>
  </property>
</configuration>
```</p>
</li>
<li>
<p>Now write the following into your <code>$HADOOP_DIR/etc/hadoop/hdfs-site.xml</code> config file: </p>
<p>```xml
&lt;?xml version="1.0" encoding="UTF-8"?&gt;
&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;
&lt;!--
  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at</p>
<pre><code>http://www.apache.org/licenses/LICENSE-2.0
</code></pre>
<p>Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License. See accompanying LICENSE file.
--&gt;</p>
<!-- Put site-specific property overrides in this file. -->
<p><configuration>
  <property>
    <name>dfs.replication</name>
    <value>1</value>
  </property>
</configuration>
```</p>
</li>
</ol>
<h3>2.2 Passwordless ssh setup</h3>
<ol>
<li>
<p>You can check if your EC2 instance has passwordless ssh to itself already set up by ssh'ing to the alias for the machine. <code>localhost</code> should be a valid alias for the loopback IP to the machine (127.0.0.1), but feel free to check in <code>/etc/hosts</code> for the loopback alias.</p>
</li>
<li>
<p>Type <code>$ ssh localhost</code>. </p>
</li>
<li>
<p>Add the loopback IP to known hosts, and type "y". </p>
</li>
<li>
<p>If permission is denied, you can setup passwordless ssh by generating an ssh key pair and copying the public key to <code>~/.ssh/authorized_keys</code>. </p>
<p><code>$ ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa</code></p>
<p><code>$ cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys</code></p>
<p><code>$ chmod 0600 ~/.ssh/authorized_keys</code></p>
</li>
</ol>
<h3>2.3 Format the NameNode</h3>
<p>The first time you set up HDFS you need to format your NameNode. We run this only once. Reformatting the namenode will wipe the entire Hadoop filesystem.</p>
<ol>
<li>
<p>Remember to <code>cd</code> into the hadoop distribution directory before running this command.</p>
<p><code>$ cd $HADOOP_DIR</code></p>
</li>
<li>
<p>Format the namenode.</p>
<p><code>$ bin/hdfs namenode -format</code></p>
</li>
</ol>
<h3>2.4 Start NameNode daemon and DataNode daemon</h3>
<ol>
<li>
<p>Type the following command to start the HDFS file system (this command wraps a bunch of subcommands to get everything set up, but there are scripts to do things more piecewise if you so desire): </p>
<p><code>$ sbin/start-dfs.sh</code></p>
</li>
</ol>
<h3>2.5 Browse NameNode web interface</h3>
<ol>
<li>
<p>If everything up to this point has gone successfully, you should be able to see a web interface view of the NameNode by pointing your browser to: </p>
<p><code>http://{Public IPv4 address}:9870/</code></p>
</li>
</ol>
<blockquote>
<p>Note: This assumes you are using port 9870 (the default) for WebHDFS</p>
</blockquote>
<ol>
<li>
<p>By default, the NameNode should be serving IP 9870 for this web interface, but if you don't see anything you may want to explicitly modify the web port property in your config files.</p>
<blockquote>
<p>If nothing shows up and you've been following this guide carefully, it is more likely something in a previous section did not run successfully.</p>
</blockquote>
</li>
<li>
<p>You should see something like this: </p>
<p><img src="img_hdfs/namenode_web_view.png"></p>
</li>
</ol>
<h3>2.6 Set up HDFS File System</h3>
<ol>
<li>
<p>Run the following commands to add a user directory with a user "hdfs_user" in your hadoop (pseudo)-distributed file system.</p>
<p><code>$ bin/hdfs dfs -mkdir /user</code></p>
<p><code>$ bin/hdfs dfs -mkdir /user/hdfs-user</code></p>
</li>
<li>
<p>Make a directory for your ec2 user username as well (probably <code>ec2-user</code>). This is the default directory for interacting with HDFS, and this directory is required for MapReduce jobs.</p>
<p><code>$ bin/hdfs dfs -mkdir /user/ec2-user</code></p>
</li>
<li>
<p>You can run most typical linux filesystem commands from the dfs subcommand. You should see the user directory you created if you run the following:</p>
<p><code>$ bin/hdfs dfs -ls</code></p>
</li>
<li>
<p>You can also now view your filesystem from the NameNode WebView by using <code>Utilities&gt;Browse the file system</code>.</p>
<p><img src="img_hdfs/web_browse_hdfs.png"></p>
</li>
</ol>
<h3>2.7 Set up WebHDFS</h3>
<p>WebHDFS is required for connection to third party applications, such as IBM DataStage. We will set up a username and password for a new HDFS user, then connect to that user via WebHDFS. Then we will be able to read/write data to our Hadoop File System from third party applications, enabling things like IBM DataStage flows.</p>
<ol>
<li>
<p>Enable HDFS by setting the dfs.webhdfs.enabled property to True in <code>$HADOOP_DIR/etc/hadoop/hdfs-site.xml</code>. This file should now look like the following: </p>
<p>```xml
&lt;?xml version="1.0" encoding="UTF-8"?&gt;
&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;
&lt;!--
  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at</p>
<pre><code>http://www.apache.org/licenses/LICENSE-2.0
</code></pre>
<p>Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License. See accompanying LICENSE file.
--&gt;</p>
<!-- Put site-specific property overrides in this file. -->
<p><configuration>
    <property>
        <name>dfs.replication</name>
        <value>1</value>
    </property>
    <property>
        <name>dfs.webhdfs.enabled</name>
        <value>true</value>
    </property>
</configuration>
```</p>
</li>
<li>
<p>Also update the <code>core-site.xml</code> file in the same config directory with hadoop.security.authentication=simple and hadoop.security.authorization=false. This xml config file should now look like the below:</p>
<p>```xml
&lt;?xml version="1.0" encoding="UTF-8"?&gt;
&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;
&lt;!--
  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at</p>
<pre><code>http://www.apache.org/licenses/LICENSE-2.0
</code></pre>
<p>Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License. See accompanying LICENSE file.
--&gt;</p>
<!-- Put site-specific property overrides in this file. -->
<p><configuration>
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://{Public IPv4 DNS}:9000</value>
    </property>
    <property>
        <name>hadoop.security.authentication</name>
        <value>simple</value>
    </property>
    <property>
        <name>hadoop.security.authorization</name>
        <value>false</value>
    </property>
</configuration>
```</p>
</li>
<li>
<p>Download Apache HTTP Server Tools.</p>
<p><code>$ sudo yum install httpd-tools -y</code></p>
</li>
<li>
<p>Make a conf folder for hadoop simple user authentication named <code>{$HADOOP_FOLDER}/etc/hadoop/conf</code>.</p>
<p><code>$ sudo mkdir etc/hadoop/conf</code></p>
</li>
<li>
<p>Make a webhdfs user on the EC2 node.</p>
<p><code>$ sudo adduser webhdfs</code></p>
</li>
<li>
<p>Add the webhdfs user to hadoop's authentication list along with a password.</p>
<p><code>$ sudo htpasswd -c etc/hadoop/conf/webhdfs.password webhdfs</code></p>
</li>
<li>
<p>Enter the password twice, and confirm. Make note of this password, which I will now refer to as <code>{webhdfs_password}</code>.</p>
</li>
<li>
<p>Make sure to add a <code>/user/username</code> folder in hdfs whenever adding a new linux/hadoop user that you want to use to login to the hadoop filesystem. In this case, you need to make a <code>/user/webhdfs</code> directory:</p>
<p><code>$ bin/hdfs dfs -mkdir /user/webhdfs</code></p>
</li>
<li>
<p>Copy in our input data file (previously added via Cyberduck) from the local node to the HDFS system:</p>
<p><code>$ bin/hdfs dfs -put ~/input/sales_data_sample.csv /user/webhdfs</code></p>
</li>
<li>
<p>Now we need to modify the hadoop access control list (ACL). If we don't do this, we will be able to log in as our <code>webhdfs</code> user and see the filesystem, but we won't be able to actually read/write data contents. To add read, write, and execute permissions to the webhdfs user in HDFS folder <code>/user/webhdfs</code>, do the following:
    &gt;Note: You must specify the -R (recursive) flag so that all files within this folder are owned by the webhdfs user. </p>
<p><code>$ bin/hdfs dfs -setfacl -R -m user:webhdfs:rwx /user/webhdfs</code></p>
</li>
<li>
<p>Now let's get the ACL info and confirm that worked correctly. 
<code>$ bin/hdfs dfs -getfacl /user/webhdfs</code></p>
<blockquote>
<p>You should see the following output (at a minimum, the line with user::webhdfs    should match):
```# file: /user/webhdfs</p>
<h1>owner: ec2-user</h1>
<h1>group: supergroup</h1>
<p>user::rwx
user:webhdfs:rwx
group::r-x
mask::rwx
other::r-x</p>
</blockquote>
</li>
<li>
<p>Verify also that <code>user::webhdfs</code> has rwx permissions on <code>/user/webhdfs/sales_data_sample.csv</code>, following the same command format.</p>
</li>
<li>
<p>Now add the properties dfs.web.authentication.kerberos.principal, dfs.web.authentication.simple.anonymous.allowed, dfs.web.authentication.simple.username, dfs.web.authentication.simple.password to your <code>hdfs-site.xml</code> config file so that it looks like the following.
      &gt; Note: If you were using Kerberos you would set up an actual Kerberos principal name, but we leave it as it is since we are using simple user/password authentication. If you named your user or password file differently, change the properties in the config as necessary.</p>
<p>```xml
  &lt;?xml version="1.0" encoding="UTF-8"?&gt;
  &lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;
  &lt;!--
    Licensed under the Apache License, Version 2.0 (the "License");
    you may not use this file except in compliance with the License.
    You may obtain a copy of the License at</p>
<pre><code>  http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License. See accompanying LICENSE file.
</code></pre>
<p>--&gt;</p>
<!-- Put site-specific property overrides in this file. -->
<p><configuration>
      <property>
        <name>dfs.replication</name>
        <value>1</value>
      </property>
      <property>
        <name>dfs.webhdfs.enabled</name>
        <value>true</value>
      </property>
      <property>
        <name>dfs.web.authentication.kerberos.principal</name>
        <value>HTTP/_HOST@EXAMPLE.COM</value>
      </property>
      <property>
        <name>dfs.web.authentication.simple.anonymous.allowed</name>
        <value>false</value>
      </property>
      <property>
        <name>dfs.web.authentication.simple.username</name>
        <value>webhdfs</value>
      </property>
      <property>
        <name>dfs.web.authentication.simple.password</name>
        <value>/etc/hadoop/conf/webhdfs.password</value>
      </property>
  </configuration>
  ```</p>
</li>
<li>
<p>Rarely, DataNodes DNS resolution goes wrong. Unfortunately, this issue plagued me for hours trying to get HDFS to allow Data I/O because data transfer was being performed on a faulty DNS resolution. If you follow this guide to a T, you may find the same issue. To prevent it, manually specify the DNS with the following properties:</p>
<p>• dfs.datanode.hostname={Public IPv4 DNS}</p>
<p>• dfs.datanode.use.datanode.hostname=true</p>
</li>
<li>
<p>Your <code>hdfs-site.xml</code> config file should now look like the following:</p>
<p>```xml
&lt;?xml version="1.0" encoding="UTF-8"?&gt;
&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;
&lt;!--
  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at</p>
<pre><code>http://www.apache.org/licenses/LICENSE-2.0
</code></pre>
<p>Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License. See accompanying LICENSE file.
--&gt;</p>
<!-- Put site-specific property overrides in this file. -->
<p><configuration>
    <property>
      <name>dfs.replication</name>
      <value>1</value>
    </property>
    <property>
      <name>dfs.webhdfs.enabled</name>
      <value>true</value>
    </property>
    <property>
      <name>dfs.web.authentication.kerberos.principal</name>
      <value>HTTP/_HOST@EXAMPLE.COM</value>
    </property>
    <property>
      <name>dfs.web.authentication.simple.anonymous.allowed</name>
      <value>false</value>
    </property>
    <property>
      <name>dfs.web.authentication.simple.username</name>
      <value>webhdfs</value>
    </property>
    <property>
      <name>dfs.web.authentication.simple.password</name>
      <value>/etc/hadoop/conf/webhdfs.password</value>
    </property>
    <property>
      <name>dfs.datanode.hostname</name>
      <value>ec2-54-67-104-53.us-west-1.compute.amazonaws.com</value>
    </property>
    <property>
      <name>dfs.datanode.use.datanode.hostname</name>
      <value>true</value>
    </property>
</configuration>
```</p>
</li>
<li>
<p>It may also be a good idea to manually specify datanode address, RPC address, HTTP address, and secondary namenode address using the following property values (to prevent other default property errors). These values use the default ports:</p>
<p>• dfs.namenode.rpc-address={Public IPv4 DNS}:9000</p>
<p>• dfs.datanode.address={Public IPv4 DNS}:50010</p>
<p>• dfs.datanode.http.address={Public IPv4 DNS}:50075</p>
<p>• dfs.secondary.http.address={Public IPv4 DNS}:50090</p>
</li>
<li>
<p>Your <code>hdfs-site.xml</code> config file should now look like the following: </p>
<p>```xml
&lt;?xml version="1.0" encoding="UTF-8"?&gt;
&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;
&lt;!--
  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at</p>
<pre><code>http://www.apache.org/licenses/LICENSE-2.0
</code></pre>
<p>Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License. See accompanying LICENSE file.
--&gt;</p>
<!-- Put site-specific property overrides in this file. -->
<p><configuration>
    <property>
      <name>dfs.replication</name>
      <value>1</value>
    </property>
    <property>
      <name>dfs.webhdfs.enabled</name>
      <value>true</value>
    </property>
    <property>
      <name>dfs.web.authentication.kerberos.principal</name>
      <value>HTTP/_HOST@EXAMPLE.COM</value>
    </property>
    <property>
      <name>dfs.web.authentication.simple.anonymous.allowed</name>
      <value>false</value>
    </property>
    <property>
      <name>dfs.web.authentication.simple.username</name>
      <value>webhdfs</value>
    </property>
    <property>
      <name>dfs.web.authentication.simple.password</name>
      <value>/etc/hadoop/conf/webhdfs.password</value>
    </property>
    <property>
      <name>dfs.namenode.rpc-address</name>
      <value>ec2-54-67-104-53.us-west-1.compute.amazonaws.com:9000</value>
    </property>
    <property>
      <name>dfs.datanode.address</name>
      <value>ec2-54-67-104-53.us-west-1.compute.amazonaws.com:50010</value>
    </property>
    <property>
      <name>dfs.datanode.http.address</name>
      <value>ec2-54-67-104-53.us-west-1.compute.amazonaws.com:50075</value>
    </property>
    <property>
      <name>dfs.secondary.http.address</name>
      <value>ec2-54-67-104-53.us-west-1.compute.amazonaws.com:50090</value>
    </property>
    <property>
      <name>dfs.datanode.hostname</name>
      <value>ec2-54-67-104-53.us-west-1.compute.amazonaws.com</value>
    </property>
    <property>
      <name>dfs.datanode.use.datanode.hostname</name>
      <value>true</value>
    </property>
</configuration>
```</p>
</li>
<li>
<p>Restart the HDFS service with the following commands:
    &gt;This should resource your config files to enable the new properties you just added.</p>
<blockquote>
<p>You must do this anytime you modify the configuration files such as <code>hdfs-site.xml</code> and <code>core-site.xml</code> if you want the changes to be reflected.</p>
</blockquote>
<p><code>$ sbin/stop-dfs.sh</code></p>
<p><code>$ sbin/start-dfs.sh</code></p>
</li>
<li>
<p>Test if WebHDFS is working. One way to do this is to submit a simple request through WebHDFS from your local computer. Open a shell process from your local computer, and run the following command, replacing user, password and public IP address as necessary:</p>
<p><code>$ curl -i -u {webhdfs username}:{password} "http://{Public IPv4 DNS}:9870/webhdfs/v1/?op=LISTSTATUS"</code></p>
<blockquote>
<p>If this runs successfully you should see an output like the following, returning a json representation of the result of your submitted filesystem command:
```HTTP/1.1 200 OK
Date: Mon, 03 Apr 2023 22:46:21 GMT
Cache-Control: no-cache
Expires: Mon, 03 Apr 2023 22:46:21 GMT
Date: Mon, 03 Apr 2023 22:46:21 GMT
Pragma: no-cache
X-Content-Type-Options: nosniff
X-FRAME-OPTIONS: SAMEORIGIN
X-XSS-Protection: 1; mode=block
Content-Type: application/json
Transfer-Encoding: chunked</p>
<p>{"FileStatuses":{"FileStatus":[
{"accessTime":0,"blockSize":0,"childrenNum":2,"fileId":16386,"group":"supergroup","length":0,&gt;"modificationTime":1680501939617,"owner":"ec2-user","pathSuffix":"user","permission":"755",&gt;"replication":0,"storagePolicy":0,"type":"DIRECTORY"}
]}}</p>
</blockquote>
</li>
</ol>
<blockquote>
<p>Note: The command provided in this step assumes you are running WebHDFS on port 9870. Modify the port number in the command if you have changed the WebHDFS port in the config files.</p>
</blockquote>
<ol>
<li>Also test whether data I/O is working correctly.<ol>
<li>
<p>Point your browser to the webview</p>
<p><code>http://{Public IPv4 address}:9870/</code>
    2.  Navigate to <code>Utilities &gt; Browse Filesystem</code>. 
    3.  Click through the filesystem until the data you uploaded <code>webhdfs/sales_data_sample.csv</code>. 
    4.  Click on the datafile, and output the head of the data.
    5.  If this runs successfully and returns the first several lines of your <code>.csv</code> file you are all set!
    6.  If not, see step #14.</p>
</li>
</ol>
</li>
</ol>
<h3>2.8 Various Gotchas to Look Out For</h3>
<ol>
<li>
<p><strong><em>The <code>setfacl</code> command in hdfs</em></strong>: </p>
<p>You can no longer need to specify the -d flag in Apache HDFS 3.0.0 and later to trigger inheritance of permissions. Supposedly access control lists (ACLs) within the directory where you <code>bin/hdfs dfs -setfacl</code> will inherit permissions by default, but that has not been my experience. </p>
<p>If you add new data into your HDFS filesystem and something doesn't run as expected, check user permissions for whichever user you are attempting to access the data from. </p>
</li>
<li>
<p>If your third party application runs slowly, you may want to increase the amount of time that  aclient will wait for a responde from a DataNode when reading data directly from the DataNode (short-circuit reads). The dfs.client.read.shortcircuit.streams.timeout property is a configuration parameter in Hadoop Distributed File System (HDFS) that specifies the maximum time in milliseconds that a client will wait for a response from the DataNode while short-circuit reading a block of data. Edit this property in <code>hdfs-site.xml</code>, if necessary. Values larger than 10000 will be brought down to 10000. Thus, below is the longest possible setting (10s).</p>
<p>```xml
...</p>
<pre><code>&lt;property&gt;
  &lt;name&gt;dfs.client.read.shortcircuit.streams.timeout&lt;/name&gt;
  &lt;value&gt;10000&lt;/value&gt;
&lt;/property&gt;
</code></pre>
<p>...
```</p>
</li>
<li>
<p>Set <code>$HADOOP_CONF_DIR</code> to make sure that configuration files are sourced correctly. If you notice that restarting the daemons via <code>sbin/stop-dfs.sh</code> and <code>sbin/start-dfs.sh</code> does not enact any changes, you may need to check your <code>$HADOOP_CONF_DIR</code> variable. Hadoop will look in this file path when sourcing its configuration.</p>
</li>
<li>
<p><code>./$HADOOP_DIR/sbin/stop-dfs.sh</code> and <code>./$HADOOP_DIR/sbin/start-dfs.sh</code> anytime you make changes to your config files, so they can be re-sourced.</p>
</li>
<li>
<p>DO NOT reformat the NameNode unless as an absolute last resort. This will wipe your entire filesystem.</p>
</li>
<li>
<p>Use the command <code>jps</code> to see running java processes. On this pseudo-distributed single-node setup you should see processes for NameNode, DataNode, and Secondary NameNode. If not, something is misconfigured. Check the error logs.</p>
</li>
<li>
<p>Error logs lie in <code>$HADOOP_DIR/logs</code>. To avoid going through the massive files, try </p>
<p><code>cat {log_file} | grep "ERROR"</code></p>
<p>and </p>
<p><code>cat {log_file} | grep "WARN"</code></p>
</li>
<li>
<p>Another useful debug strategy is to use the </p>
</li>
</ol>
<p><code>$ $HADOOP_DIR/bin/hdfs dfsadmin -report</code> </p>
<p>command to see a report on the active Apache HDFS environment. For a setup as described in this guide, you should see something like the following in your output. If no DataNode shows up or the IP address is completely wrong, check your configuration settings on properties relating to the datanode address and datanode DNS resolution.</p>
<pre><code>```
Configured Capacity: 8577331200 (7.99 GB)
Present Capacity: 4329267200 (4.03 GB)
DFS Remaining: 4329246720 (4.03 GB)
DFS Used: 20480 (20 KB)
DFS Used%: 0.00%
Replicated Blocks:
    Under replicated blocks: 0
    Blocks with corrupt replicas: 0
    Missing blocks: 0
    Missing blocks (with replication factor 1): 0
    Low redundancy blocks with highest priority to recover: 0
    Pending deletion blocks: 0
Erasure Coded Block Groups:
    Low redundancy block groups: 0
    Block groups with corrupt internal blocks: 0
    Missing block groups: 0
    Low redundancy blocks with highest priority to recover: 0
    Pending deletion blocks: 0

-------------------------------------------------
Live datanodes (1):

Name: 172.31.10.162:50010 (ip-172-31-10-162.us-west-1.compute.internal)
Hostname: ec2-54-67-104-53.us-west-1.compute.amazonaws.com
Decommission Status : Normal
Configured Capacity: 8577331200 (7.99 GB)
DFS Used: 20480 (20 KB)
Non DFS Used: 4248064000 (3.96 GB)
DFS Remaining: 4329246720 (4.03 GB)
DFS Used%: 0.00%
DFS Remaining%: 50.47%
Configured Cache Capacity: 0 (0 B)
Cache Used: 0 (0 B)
Cache Remaining: 0 (0 B)
Cache Used%: 100.00%
Cache Remaining%: 0.00%
Xceivers: 0
Last contact: Thu Apr 06 06:02:30 UTC 2023
Last Block Report: Thu Apr 06 06:02:19 UTC 2023
Num of Blocks: 1

```
</code></pre>
<ol>
<li>Any time you restart your EC2 node its Public DNS and Public IP will change. This means you need to edit your hadoop config files upon each EC2 restart.</li>
</ol>
<h2>3. Connect CP4D to Hadoop HDFS</h2>
<p>Congratulations! You should now have a fully functioning pseudo-distributed HDFS node with WebHDFS capabilities, and you're ready to connect HDFS with Third Party applications. We will connect to IBM's Cloud Pak 4 Data (CP4D).  </p>
<h3>3.1. Configure a Connection in the CP4D Project</h3>
<ol>
<li>
<p>Your connection details for the completed Apache HDFS system should be as follows:
   <code>username = {webhdfs user}
    uri = http://{Public IPv4 DNS}:9870/webhdfs/v1/
    {webhdfs user password}</code></p>
</li>
<li>
<p>Log into <a href="https://cloud.ibm.com/">IBM Cloud</a>.</p>
</li>
<li>
<p>Launch the IBM Cloud Pak 4 Data (CP4D) dashboard. For example, by navigating to <code>Resource List &gt; Watson Studio &gt; Launch in Cloud Pak 4 Data</code>.</p>
</li>
<li>
<p>Navigate to <code>Projects</code> in the dropdown sidebar.</p>
<p><img src="img_hdfs/sidebar-projects.png" width = "35%"></p>
</li>
<li>
<p>Open a project, if one is already created for your data pipelines. </p>
</li>
</ol>
<p><img alt="img" src="img_hdfs/open-project.png" /></p>
<ol>
<li>If you do not have a data pipeline project yet:</li>
<li>Click <code>New Project</code></li>
<li>Give your project a name</li>
<li>
<p>Open the project.</p>
</li>
<li>
<p>Let's add a Connection for the Apache HDFS system we architected in the previous sections.</p>
</li>
<li>
<p>Click <code>New Asset</code>.</p>
<p><img src="img_hdfs/new-asset.png" width = "40%"></p>
</li>
<li>
<p>Select <code>Connection</code> for the asset type.</p>
<p><img src="img_hdfs/new-asset-cxn.png" width = "50%"></p>
</li>
<li>
<p>Search "Apache" to find the Apache HDFS connection type. Select <code>Apache HDFS</code> for the connection type.</p>
<p><img src="img_hdfs/apache-hdfs-asset.png" width="70%"></p>
</li>
<li>
<p>Configure the connection with the details from step #1. </p>
<p><img src="img_hdfs/cxn-setup.png" width="70%"></p>
</li>
<li>
<p>Click <code>Test Connection</code>. If the test is successful, move to the next section to build your data flow in DataStage.</p>
</li>
<li>
<p>If the test is unsuccessful:</p>
<ol>
<li>Check the connection details you provided</li>
<li>Make sure your WebHDFS port matches</li>
<li>Check that your AWS EC2 node is up and running</li>
<li>Recheck that you can access your Apache HDFS system's webview on port 9870 through your broswer</li>
<li>Recheck that HTTP requests work with your Apache HDFS system</li>
</ol>
</li>
</ol>
<h2>4. Build a Data Flow that Performs I/O with Apache HDFS</h2>
<h3>4.1 Build a Data Flow</h3>
<ol>
<li>From your project, click <code>New Asset</code>.</li>
</ol>
<p><img src="img_hdfs/new-asset.png" width = "40%"></p>
<ol>
<li>
<p>From <code>Graphical builders</code>, choose DataStage.</p>
<p><img src="img_hdfs/choose-datastage.png"></p>
</li>
<li>
<p>Give your DataStage asset a name, then open it.</p>
</li>
<li>Our flow is going to consist of an Input step loading data in from Apache HDFS, a Transform step where we filter the data to only include big sales, and then an Output step loading the filtered data out into Apache HDFS. In this way, we mimic a very basic ETL pipeline.</li>
<li>
<p>Drag two "Apache HDFS" <code>Connectors</code> into the flow building space.</p>
<p><img src="img_hdfs/hdfs-connector.png" width="50%"></p>
</li>
<li>
<p>Drag a "Filter" <code>Stage</code> into the flow building space.</p>
<p><img src="img_hdfs/filter-stage.png" width="50%"></p>
</li>
<li>
<p>Your unconnected flow should look something like this:</p>
<p><img src="img_hdfs/unconnected-flow.png"></p>
</li>
<li>
<p>Hover over each icon and drag the flow arrows to create a connected flow from HDFS -&gt; Filter -&gt; HDFS.</p>
<p><img src="img_hdfs/connected-flow.png"></p>
</li>
<li>
<p>Feel free to give each step and link a unique name to make your flow more readable.</p>
</li>
<li>Double click on the first Apache HDFS stage.</li>
<li>
<p>Edit the stage settings to pull data from the HDFS Connection configured in the previous section. Also include the file name you want to input into your flow.</p>
<p><img src="img_hdfs/apache-in-stage-settings.png" width="70%"></p>
</li>
<li>
<p>To make sure the header (column names row) of your csv is handled correctly, check the <code>File format properties</code> settings and make sure they match the below.</p>
<p><img src="img_hdfs/file-format-properties.png" width="40%"></p>
</li>
<li>
<p>Now let's edit the <code>Output</code> settings for the HDFS input stage. We need to make sure we output all of the columns we want from our sample sales data. The easiest way to do this is to import the columns from the csv.</p>
<p><img src="img_hdfs/import-columns.png" width="50%"></p>
</li>
<li>
<p>Click on your connection, navigate to the sales data csv file, then import all columns.</p>
</li>
<li>
<p>You should now see that the columns have been imported into your first Apache HDFS stage's <code>Output</code> settings.</p>
<p><img src="img_hdfs/columns-imported.png" width="70%"></p>
</li>
<li>
<p>Click <code>Save</code> to save your changes, then go back to the flow builder space.</p>
</li>
<li>
<p>Now let's configure our <code>Filter</code> step. Double click on the icon for your <code>Filter</code> step.</p>
</li>
<li>
<p>Under <code>Stage</code> click <code>Edit</code> then add a WHERE clause which filters out any data with a sales amount less than $5000. The WHERE clause functions just as you would expect if you are familiar with SQL. </p>
<p><img src="img_hdfs/edit-filter-stage.png"></p>
</li>
<li>
<p>Click <code>Save</code> and return to the flow builder space.</p>
</li>
<li>
<p>Finally, double click on the second Apache HDFS step (the output step) and configure its <code>Properties</code>.</p>
<ol>
<li>Configure it to have the same Apache HDFS connection as the first HDFS step. We will save data back to the same Apache HDFS instance for simplicity. </li>
<li>Give your output data a name. Here I use <code>sale_data_bigsalesonly.csv</code>.</li>
<li>Make sure the <code>First line is header</code> setting is checked.</li>
</ol>
<p><img src="img_hdfs/apache-out-stage.png" width="70%"></p>
</li>
<li>
<p>Click <code>Save</code>.</p>
</li>
<li>Your flow should now be ready to go. Click <code>Compile</code>. If anything goes wrong, check the logs to see what you're missing or go back through the guide and make sure you completed all the steps and saved your changes.</li>
<li>Click <code>Run</code>. Again, if something goes wrong, check the logs.</li>
<li>
<p>When your flow has successfully run you should see something like the following. If you are using the exact same dataset and WHERE clause, you should have 2,823 input rows which get filtered to 549 "big sales" rows.</p>
<p><img src="img_hdfs/flow-after-running.png"></p>
</li>
</ol>
<h3>4.2 Verify Successful ETL</h3>
<ol>
<li>
<p>SSH back into your Apache HDFS NameNode.</p>
<p><code>$ ssh -i .ssh/hdfs-key1.pem ec2-user@$HDFS_NODE</code></p>
</li>
<li>
<p>Navigate to the hadoop folder.</p>
<p><code>$ cd $HADOOP_DIR</code></p>
</li>
<li>
<p>Move the output data into the local node's <code>output</code> folder.</p>
<p><code>$ bin/hdfs dfs -get /user/webhdfs/sale_data_bigsalesonly.csv ~/output</code></p>
</li>
<li>
<p>Feel free to compare the files from the command-line. For something easier on the eyes, consider downloading the data back to your local computer and opening the CSVs in a GUI application such as <code>Microsoft Excel</code> or <code>Numbers</code>.</p>
</li>
<li>
<p>Using Cyberduck, connect to the Apache HDFS node again. Drag the input and output <code>.csv</code> files to your local computer. If you prefer to work from the command-line use something like <code>scp</code>.</p>
</li>
<li>
<p>Here is an image of my sales data before, collected from HDFS:</p>
<p><img src="img_hdfs/sales_data_before.png">
7. Here is an image of my sales data after, collected from HDFS. Note that the data is the same, but only big sales (with <code>SALES</code> amounts &gt; 5000) have been retain.</p>
<p><img src="img_hdfs/sales_data_after.png">
8. Thus, we see that the ETL flow was successful in Data I/O and transformation with Apache HDFS. We have successfully connected CP4D's DataStage to Apache HDFS on AWS!</p>
</li>
</ol>
<p>&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;</p>
<p>&lt;&gt; Congratulations on Completing this Tutorial! &lt;&gt;</p>
<p>&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;</p>
<p>If you have any questions, please reach out to Daniel.Frees@ibm.com.</p>